# -*- coding: utf-8 -*-
"""AI Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p3yxsVBRtSQFLgwxBuVQxl4GBHXtjDIg
"""

import pandas as pd
import random

issues = [
    "Login issue",
    "Payment failed",
    "Account blocked",
    "Refund delayed",
    "Card not working",
    "Transaction not reflected"
]

channels = ["Chat", "Email", "Call"]
priorities = ["Low", "Medium", "High"]

data = []

for i in range(300):
    data.append({
        "Ticket_ID": i+1,
        "Issue_Text": random.choice(issues),
        "Channel": random.choice(channels),
        "Priority": random.choice(priorities),
        "Resolution_Time": random.randint(1,5)
    })

df = pd.DataFrame(data)
df.head()

print(df.head())
print(df.info())

print(df['Issue_Text'].value_counts().head(10))

print(df['Priority'].value_counts())

import re

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z ]', '', text)
    return text

df['clean_text'] = df['Issue_Text'].apply(clean_text)

#sentiment analysis

pip install textblob

from textblob import TextBlob

def get_sentiment(text):
    return TextBlob(text).sentiment.polarity

df['sentiment'] = df['clean_text'].apply(get_sentiment)

#identifying high-risk issues

def risk_level(row):
    if row['Priority'] == 'High' and row['sentiment'] < 0:
        return "High Risk"
    elif row['Priority'] == 'Medium':
        return "Medium Risk"
    else:
        return "Low Risk"

df['Risk_Level'] = df.apply(risk_level, axis=1)

print(df['Risk_Level'].value_counts())

#visualising

import matplotlib.pyplot as plt

df['Risk_Level'].value_counts().plot(kind='bar')
plt.title("Risk Distribution of Customer Issues")
plt.show()

df.to_csv("final_ai_insights.csv", index=False)

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(df['clean_text'])

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=5, random_state=42)
df['Cluster'] = kmeans.fit_predict(X)

print(df.groupby('Cluster')['Issue_Text'].head(3))

summary_data = df.groupby('Cluster').agg({
    'Issue_Text':'count',
    'sentiment':'mean'
}).reset_index()

summary_data.columns = ['Cluster','Total_Issues','Avg_Sentiment']

print(summary_data)

def generate_insight(row):
    if row['Avg_Sentiment'] < 0:
        tone = "negative customer sentiment"
    else:
        tone = "neutral or positive sentiment"

    return f"Cluster {row['Cluster']} has {row['Total_Issues']} issues showing {tone}, indicating potential process improvement opportunities."

summary_data['AI_Insight'] = summary_data.apply(generate_insight, axis=1)

print(summary_data[['Cluster','AI_Insight']])

high_risk = df[df['Risk_Level']=="High Risk"].shape[0]
total = df.shape[0]

executive_summary = f"""
AI Analysis Summary:
- Total issues analyzed: {total}
- High-risk issues detected: {high_risk}
- Major patterns were automatically identified using NLP clustering.
- Key recommendation: Focus on clusters showing negative sentiment trends.
"""

print(executive_summary)

#ai assistant

!pip install transformers

from transformers import pipeline

ai_assistant = pipeline("text-generation", model="gpt2")

context = summary_data.to_string()

def ask_ai(question):
    prompt = f"""
You are an AI Operations Analyst.

Here is the analysis data:
{context}

Question: {question}

Answer:
"""

    response = ai_assistant(
        prompt,
        max_length=200,
        num_return_sequences=1,
        temperature=0.7
    )

    return response[0]['generated_text']

print(ask_ai("What is the biggest risk area?"))



